# Copyright (c) 2013-2020 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (application.conf.example) contains a template with
# configuration options for Stream Enrich.

enrich {

 streams {

   in {
     # Stream/topic where the raw events to be enriched are located
     raw = "{{enrichStreamsInRaw}}"
   }

   out {
     # Stream/topic where the events that were successfully enriched will end up
     enriched = "{{enrichStreamsOutEnriched}}"
     # Stream/topic where the event that failed enrichment will be stored
     bad = "{{enrichStreamsOutBad}}"
     # Stream/topic where the pii tranformation events will end up
     pii = "stream-enrich-pii"

     # How the output stream/topic will be partitioned.
     # Possible partition keys are: event_id, event_fingerprint, domain_userid, network_userid,
     # user_ipaddress, domain_sessionid, user_fingerprint.
     # Refer to https://github.com/snowplow/snowplow/wiki/canonical-event-model to know what the
     # possible parittion keys correspond to.
     # Otherwise, the partition key will be a random UUID.
     # Note: Nsq does not make use of partition key.
     partitionKey = "event_id"
   }

   # Configuration shown is for Kafka, to use another uncomment the appropriate configuration
   # and comment out the other
   # To use stdin, comment or remove everything in the "enrich.streams.sourceSink" section except
   # "enabled" which should be set to "stdin".
   sourceSink {
     # Sources / sinks currently supported are:
     # 'kinesis' for reading Thrift-serialized records and writing enriched and bad events to a
     # Kinesis stream
     # 'kafka' for reading / writing to a Kafka topic
     # 'nsq' for reading / writing to a Nsq topic
     # 'stdin' for reading from stdin and writing to stdout and stderr
     enabled =  "kinesis"

     # Region where the streams are located (AWS region, pertinent to kinesis sink/source type)
     region = "{{enrichStreamsRegion}}"

     ## Optional endpoint url configuration to override aws kinesis endpoints,
     ## this can be used to specify local endpoints when using localstack
     # customEndpoint = {{kinesisEndpoint}}

     ## Optional endpoint url configuration to override aws dyanomdb endpoints for Kinesis checkpoints lease table,
     ## this can be used to specify local endpoints when using Localstack
     # dynamodbCustomEndpoint = "http://localhost:4569"

     # Optional override to disable cloudwatch
     # disableCloudWatch = true

     # AWS credentials
     # If both are set to 'default', use the default AWS credentials provider chain.
     # If both are set to 'iam', use AWS IAM Roles to provision credentials.
     # If both are set to 'env', use env variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
     aws {
        accessKey = "iam"
        secretKey = "iam"
     }



     # Maximum number of records to get from Kinesis per call to GetRecords
     maxRecords = 10000

     # LATEST: most recent data.
     # TRIM_HORIZON: oldest available data.
     # "AT_TIMESTAMP": Start from the record at or after the specified timestamp
     # Note: This only effects the first run of this application on a stream.
     # (pertinent to kinesis source type)
     initialPosition = "TRIM_HORIZON"

     # Need to be specified when initial-position is "AT_TIMESTAMP".
     # Timestamp format need to be in "yyyy-MM-ddTHH:mm:ssZ".
     # Ex: "2017-05-17T10:00:00Z"
     # Note: Time need to specified in UTC.
     #initialTimestamp = "{{initialTimestamp}}"

     # Minimum and maximum backoff periods, in milliseconds
     backoffPolicy {
       minBackoff = {{enrichStreamsOutMinBackoff}}
       maxBackoff = {{enrichStreamsOutMaxBackoff}}
     }
  }



   # After enrichment, events are accumulated in a buffer before being sent to Kinesis/Kafka.
   # Note: Buffering is not supported by NSQ.
   # The buffer is emptied whenever:
   # - the number of stored records reaches recordLimit or
   # - the combined size of the stored records reaches byteLimit or
   # - the time in milliseconds since it was last emptied exceeds timeLimit when
   #   a new event enters the buffer
   buffer {
     byteLimit = {{enrichStreamsBufferByteThreshold}}
     recordLimit = {{enrichStreamsBufferRecordThreshold}} # Not supported by Kafka; will be ignored
     timeLimit = {{enrichStreamsBufferTimeThreshold}}
   }

   # Used for a DynamoDB table to maintain stream state.
   # Used as the Kafka consumer group ID.
   # Used as the Google PubSub subscription name.
   appName = "{{enrichStreamsAppName}}"
 }

 # The setting below requires an adapter being ready, i.e.: https://github.com/snowplow-incubator/remote-adapter-example
 # remoteAdapters = [
 #    {
 #        vendor: "com.globeandmail"
 #        version: "v1"
 #        url: "http://remote-adapter-example:8995/sampleRemoteAdapter"
 #        connectionTimeout: 1000
 #        readTimeout: 5000
 #    }
 # ]

 # Optional section for tracking endpoints
 #monitoring {
 #  snowplow {
 #    collectorUri = "{{collectorUri}}"
 #    collectorUri = ${?ENRICH_MONITORING_COLLECTOR_URI}
 #    collectorPort = 80
 #    collectorPort = ${?ENRICH_MONITORING_COLLECTOR_PORT}
 #    appId = {{enrichAppName}}
 #    appId = ${?ENRICH_MONITORING_APP_ID}
 #    method = GET
 #    method = ${?ENRICH_MONITORING_METHOD}
 #  }
 #}
}
